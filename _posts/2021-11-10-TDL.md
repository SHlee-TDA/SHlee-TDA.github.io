---
title: Topological Approaches to Deep Learning
tags: [Mathematics, Deep Learning]
mathjax: true
use_math: true
---
# Topological Approaches to Deep Learning

Author : Gunnar Carlsson et al.

Translator : Seong Heon Lee

Link : [Click](https://arxiv.org/pdf/1811.01122.pdf)

---

# 1. Introduction




딥 뉴럴 네트워크는 거대하고 복잡한 데이터 세트으로 문제를 해결하는 데 있어서 강력하고 매력적인 방법이다. 

이러한 방법은 매우 큰 계산에 대한 틀로서 무방향 그래프(undirected graph)를 사용한하고, 이미지, 텍스트, 시계열 등 많은 종류의 데이터에 대한 연구에서 수많은 성과를 보여왔다.

그러나 이들의 응용 가능성에 제약을 거는 한 가지 이슈는 그것이 어떻게 작동하는지에 대해서 상세하게 이해되지 않는다는 사실이다.

관련된 문제로는 특정 데이터 세트에서 종종 어떤 종류의 과적합이 생기곤 하는데, 이러한 결과로 소위 적대적 행동(adversarial behavior)이라 불리는 것의 발생 가능성을 야기한다. 

여기서 적대적 행동은 예를 들어 이미지 데이터에 대부분의 사람들은 인지하지 못할 정도의 매우 작은 변화를 주는 것에 의해 머신이 잘못된 결과를 예측하는 현상 따위를 말한다.

이러한 이유들 때문에, 뉴럴 네트워크들의 내부 상태에 대하여 이해를 더하는 방법의 개발은 매우 바람직하다.
매우 많은 수의 노드들(또는 뉴런들), 그리고 네트워크를 학습시키는데 사용되는 최적화 알고리즘의 스토캐스틱한 성질 때문에 이것은 데이터 분석 분야, 특별히 비지도 데이터 분석에 대한 문제다.

본 논문에서 연구의 초기 목표는 TDA가 포유류 시각 경로의 알려진 속성을 어느 정도 재현하는지 이해할 뿐만 아니라 이러한 종류의 통찰력을 제공할 수 있음을 입증하기 위해 이미지 데이터에 대해 훈련되는 신경망의 내부 상태에 대해 위상수학적 데이터 분석(TDA)을 수행하는 것이었다.
우리는 이런 분석을 수행했고 그 결과는 섹선 4에 보고되어있다.

우리는 우리의 발견이 [2]에서 얻은 자연적 이미지들의 이미지 패치들에 대한 데이터 분석적 결과들에 꽤 일맥상통하다는 것을 보인다.

또한, 우리는 한 가지 예에서 학습 프로세스를 연구할 수 있으며, 네트워크의 다른 계층이 수행하는 역할을 명확히 하는 흥미로운 결과와 함께 매우 심층적으로 사전 훈련된 신경망을 연구할 수 있다.







이런 실험들이 수행됨에 따라, 우리는 우리의 연구로부터 얻은 지식을 더욱 일반적으로 딥 러닝에 어떻게 적용시킬 수 있을지에 대한 질문에 흥미를 가지게 되었다.

특히, 우리는 다른 커다란 클래스의 데이터 세트나 개별 데이터 세트들에 잘 적용되는 효과적인 네트워크를 구축할 방법을 얻기 위해서는 어떻게 컨볼루셔널 뉴럴 넷 (CNN)의 구조를 다른 데이터 세트에 대해 일반화 시켜야 할지 물었다.

우리는 이미지 CNN 구조로부터 얻은 핵심 아이디어는 특징들(픽셀들)의 집합에 거리함수로 표현될 수 있는 기하학이 내포되어 있다는 사실이며, 이는 일반적으로 배열된 픽셀들이 있는 그리드로부터 나타남을 알았다.

그러나 대부분의 데이터 세트에서 특징 간 거리에 대한 하나 이상의 자연적 개념을 가지고 있으며, 그러한 메트릭에 기초한 일반화는 그러한 메트릭에 기초한 연결에 제한을 둔 신경망을 구축하는 매우 강력한 방법의 원천으로 보인다.

특징들의 기하학적 성질들을 연구하는 아이디어는 *위상적 신호 처리(topological signal processing)*이라는 표제 아래 M. Robinson [11]에 의해 예견됐다.

그래서 이 논문에서 우리의 두 번째 목표는 데이터 세트의 특성 공간에 대한 정보를 기반으로 하여 거리함수와 그래프로부터 뉴럴 네트워크 구조를 구축하는 것에 대한 수학적 형식화를 소개하는 것이다.

우리는 또한 이러한 형식화가 표준적인 방법을 사용하더라도 뉴럴 네트워크의 설명을 간결하고 정확하게 만든다는 것을 알았다.

섹션 5.2에서 이러한 아이디어의 가장 간단한 응용으로부터 가능한 성능 향상을 평가한다.

이러한 성능 향상의 본질은 두 가지 방향에서 온다.

첫 번째 종류의 개선은 학습 과정의 속도 향상의 방향이다. 

뉴럴 넷의 훈련은 꽤나 시간을 잡아먹는 작업이고, 훈련의 비용(시간)을 줄이는 것은 명백히 바람직하다.

우리는 이 방법이 더 복잡한 데이터셋에서 더 효과적이라는 것을 알게 되었고, 이는 고무적인 일입니다. 

두 번째 종류의 개선은 일반화의 방향이다.

이미지 데이터 세트에 대한 학습을 수행할 때는 데이터 세트의 두 부분집합(훈련 세트와 테스트 세트)을 선택하는 것이 표준 절차다. 

네트워크는 훈련 세트에 대해 학습되고 정확도는 테스트 세트에 대해 평가된다.

 이 절차는 과적합을 방지하기 위해 고안되었으며 정확도는 종종 매우 인상적인 수치를 달성한다.

 그러나 하나의 데이터 세트에 대한 학습과 완전히 다른 데이터 세트에 대한 평가 문제를 고려할 수 있다. 

예를 들어, 숫자 이미지로로 구성된 두 개의 유명한 데이터 세트들이 있다. 

하나는 MNIST[7]이고 다른 하나는 SVHN[16]이다.

첫 번째는 비교적 "깨끗한" 데이터 세트이고, 두 번째는 실제로 주택 주소에 대한 숫자의 이미지에서 얻는다. 

혹자는 MNIST에서 훈련하고 MNIST의 다른 부분 집합이 아닌 SVHN에서 정확도를 평가하려고 시도할 수 있다. 

놀랍게도, 이 과정의 결과는 무작위 분류 선택으로 얻은 정확도에 가까운 최악의 결과를 산출한다. 

우리는 우리가 논의한 방법을 사용함으로써 여전히 허용 가능한 수준은 아니지만 정확도를 크게 향상시킬 수 있음을 입증한다.

이는 이 방법을 더 적용하면 일반화가 훨씬 향상될 수 있음을 시사한다.








우리는 특성 공간(feature space)에 대한 기하학적 정보를 생성하는 세 가지 시나리오를 상정한다.

첫 번째는 구조상 일련의 특징들이 기하학적 구조를 갖추고 있다는 것이다. 

이러한 상황의 일반적인 예는 이미지 또는 시계열이다. 

예를 들어 픽셀(이미지의 특성)은 직사각형 기하학을 염두에 두고 설계되었다. 

두 번째는 [2]에서 수행한 것과 같은 연구에서 기하학이 얻어지는 경우이다.

마지막으로, 숫자 성분을 가지는 다소 일반화된 데이터 행렬이 주어지고 여기에 유클리디언, 헤밍 등의 표준적인 거리의 선택을 통해 거리 함수를 부여한 상황이 있다.

일단 이 작업이 완료되면, 이러한 기하학적 정보를 매퍼(Mapper)[12]의 구축과 같은 방식을 통해 더 작은 표현으로 압축할 수 있는 것이 중요하다[12].

우리는 다양한 종류의 데이터 세트에 부착된 특성 공간의 기하학적 구조를 연구하는 것이 신경망의 구성에 정보를 제공하고 성능을 향상시킬 수 있는 매우 강력한 도구가 될 것이라고 믿는다. 

또한, 우리가 기하학적 방법을 구조에 포함시켰기 때문에, 우리는 또한 우리의 형식주의가 신경망에 대한 더 정교하고 상세하며 미묘한 수학적 분석의 문을 연다고 믿는다.




# 2. Neural Nets





이 섹션에서는 컨볼루셔널 뉴럴 네트워크(CNN)의 특별한 종류와 같은 피드-포워드(feed-forward) 뉴럴 네트워크를 소개할 것이다.



---

**Definition 2.1**

깊이 $r$의 *피드-포워드 시스템*이란, 꼭짓점 집합이 $V(\Gamma)$로 주어진 비순환 유향 그래프(directed acyclic graph) $\Gamma$가 다음 조건들을 만족시키는 것을 말한다.

1. $V(\Gamma)$는 다음의 서로소인 합집합으로 분해된다.
    $$V(\Gamma) = V_0(\Gamma) \sqcup V_1(\Gamma) \sqcup \cdots \sqcup V_r(\Gamma)$$

2. 만약 $v\in V_i(\Gamma)$이면, $\Gamma$에 존재하는 모든 $(v,w)$ 꼴의 변은 $w \in V_{i+1}(\Gamma)$를 끝점으로 가진다.

3. $V_0(\Gamma)$의 노드들은 *이니셜 노드*라고 불리고, $V_r(\Gamma)$의 노드들은 *터미널 노드*라고 불린다.

4. 모든 비-이니셜 노드 $w\in V_i(\Gamma)$에 대하여, 적어도 하나의 노드 $v \in V_{i-1}(\Gamma)$가 존재하여 $(v,w)$가 $\Gamma$의 한 변이 됨을 가정한다.

5. $\Gamma$의 각 꼭짓점 $v$에 대하여, $\Gamma(v)$는 $(v,w)$가 $\Gamma$의 변이 되는 모든 $\Gamma$의 꼭짓점 $w$의 집합을 나타낸다. $\Gamma^{-1}(v)$는 $(w,v)$가 $\Gamma$의 변이 되는 모든 $\Gamma$의 꼭짓점 $w$의 집합을 나타낸다.


---







집합 $V_i(\Gamma)$는 피드-포워드 시스템의 *레이어*라고 부른다.

레이어 $V_i(\Gamma)$가 *국소적으로 유한(locally finite)*하다는 것은 집합 $\Gamma^{-1}(v)$들이 모든 $v\in V_i(\Gamma)$에 대하여 유한집합일때를 말한다.

깊이 $r$인 피드-포워드 시스템 $\Gamma$의 *서브-피드-포워드 시스템*이란, 그래프 $\Gamma_0$와 꼭짓점들의 족 $V_0(\Gamma) \cap \Gamma_0, \ldots, V_r(\Gamma)\cap \Gamma_0$들이 하나의 피드-포워드 시스템을 형성하는 유향 부분그래프 $\Gamma_0 \subseteq \Gamma$를 말한다.

특별히 이 경우 각 $v\in \Gamma_0$에 대하여, 집합 $\Gamma^{-1}(v) \cap \Gamma_0$는 공집합이 아니어야만 한다.
